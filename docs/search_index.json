[["байесовский-регрессионный-анализ.html", "11 Байесовский регрессионный анализ 11.1 Основы регрессионного анализа 11.2 brms", " 11 Байесовский регрессионный анализ library(tidyverse) 11.1 Основы регрессионного анализа Когда мы используем регрессионный анализ, мы пытаемся оценить два параметра: свободный член (intercept) – значение \\(y\\) при \\(x = 0\\); угловой коэффициент (slope) – изменение \\(y\\) при изменении \\(x\\) на одну единицу. \\[y_i = \\beta_0 + \\beta_1\\times x_i + \\epsilon_i\\] Причем, иногда мы можем один или другой параметр считать равным нулю. При этом, вне зависимости от статистической школы, у регрессии есть свои ограничения на применение: линейность связи между \\(x\\) и \\(y\\); нормальность распределение остатков \\(\\epsilon_i\\); гомоскидастичность — равномерность распределения остатков на всем протяжении \\(x\\); независимость переменных; независимость наблюдений друг от друга. 11.2 brms Для анализа возьмем датасет, который я составил из UD-корпусов и попробуем смоделировать связь между количеством слов в тексте и количеством уникальных слов (закон Хердана-Хипса). ud &lt;- read_csv(&quot;https://raw.githubusercontent.com/agricolamz/udpipe_count_n_words_and_tokens/master/filtered_dataset.csv&quot;) glimpse(ud) ## Rows: 20,705 ## Columns: 5 ## $ doc_id &lt;chr&gt; &quot;KR1d0052_001&quot;, &quot;KR1d0052_002&quot;, &quot;KR1d0052_003&quot;, &quot;KR1d0052_… ## $ n_words &lt;dbl&gt; 3516, 2131, 4927, 4884, 4245, 5027, 3406, 2202, 2673, 2300… ## $ n_tokens &lt;dbl&gt; 842, 546, 869, 883, 737, 1085, 494, 443, 573, 578, 660, 87… ## $ language &lt;chr&gt; &quot;Classical_Chinese&quot;, &quot;Classical_Chinese&quot;, &quot;Classical_Chine… ## $ corpus_code &lt;chr&gt; &quot;Kyoto&quot;, &quot;Kyoto&quot;, &quot;Kyoto&quot;, &quot;Kyoto&quot;, &quot;Kyoto&quot;, &quot;Kyoto&quot;, &quot;Kyo… Для начала, нарушим кучу ограничений на применение регрессии и смоделируем модель для вот таких вот данных, взяв только тексты меньше 1500 слов: ud %&gt;% filter(n_words &lt; 1500) -&gt; ud ud %&gt;% ggplot(aes(n_words, n_tokens))+ geom_point() 11.2.1 Модель только со свободным членом library(brms) get_prior(n_tokens ~ 1, data = ud) Вот модель с встроенными априорными распределениями: fit_intercept &lt;- brm(n_tokens ~ 1, data = ud, silent = TRUE) ## ## SAMPLING FOR MODEL &#39;65fac4710f6791442452f18e53c1ca6b&#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 0.000367 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 3.67 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 1: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 1: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 1: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 1: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 1: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 1: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 1: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 1: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 1: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 1: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 1: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 2.05836 seconds (Warm-up) ## Chain 1: 1.03635 seconds (Sampling) ## Chain 1: 3.09471 seconds (Total) ## Chain 1: ## ## SAMPLING FOR MODEL &#39;65fac4710f6791442452f18e53c1ca6b&#39; NOW (CHAIN 2). ## Chain 2: ## Chain 2: Gradient evaluation took 0.000278 seconds ## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 2.78 seconds. ## Chain 2: Adjust your expectations accordingly! ## Chain 2: ## Chain 2: ## Chain 2: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 2: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 2: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 2: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 2: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 2: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 2: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 2: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 2: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 2: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 2: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 2: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 2: ## Chain 2: Elapsed Time: 1.42631 seconds (Warm-up) ## Chain 2: 0.998136 seconds (Sampling) ## Chain 2: 2.42445 seconds (Total) ## Chain 2: ## ## SAMPLING FOR MODEL &#39;65fac4710f6791442452f18e53c1ca6b&#39; NOW (CHAIN 3). ## Chain 3: ## Chain 3: Gradient evaluation took 0.000288 seconds ## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 2.88 seconds. ## Chain 3: Adjust your expectations accordingly! ## Chain 3: ## Chain 3: ## Chain 3: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 3: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 3: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 3: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 3: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 3: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 3: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 3: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 3: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 3: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 3: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 3: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 3: ## Chain 3: Elapsed Time: 1.30875 seconds (Warm-up) ## Chain 3: 0.948708 seconds (Sampling) ## Chain 3: 2.25746 seconds (Total) ## Chain 3: ## ## SAMPLING FOR MODEL &#39;65fac4710f6791442452f18e53c1ca6b&#39; NOW (CHAIN 4). ## Chain 4: ## Chain 4: Gradient evaluation took 0.000274 seconds ## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 2.74 seconds. ## Chain 4: Adjust your expectations accordingly! ## Chain 4: ## Chain 4: ## Chain 4: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 4: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 4: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 4: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 4: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 4: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 4: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 4: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 4: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 4: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 4: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 4: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 4: ## Chain 4: Elapsed Time: 1.59419 seconds (Warm-up) ## Chain 4: 1.17064 seconds (Sampling) ## Chain 4: 2.76483 seconds (Total) ## Chain 4: При желании встроенные априорные расспеределения можно не использовать и вставлять в аргумент prior априорные распределения по вашему желанию. fit_intercept ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: n_tokens ~ 1 ## Data: ud (Number of observations: 20282) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 135.64 0.87 133.95 137.36 1.00 2938 2241 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 121.76 0.60 120.62 122.97 1.00 4202 2792 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). plot(fit_intercept) Взято остюда: 11.2.1.1 R-hat R-hat convergence diagnostic compares the between- and within-chain estimates for model parameters and other univariate quantities of interest. If chains have not mixed well (ie, the between- and within-chain estimates don’t agree), R-hat is larger than 1. We recommend running at least four chains by default and only using the sample if R-hat is less than 1.01. Stan reports R-hat which is the maximum of rank normalized split-R-hat and rank normalized folded-split-R-hat, which works for thick tailed distributions and is sensitive also to differences in scale. For more details on this diagnostic, see https://arxiv.org/abs/1903.08008. Recommendations: Look at Bulk- and Tail-ESS for further information. Look at the rank plot to see how the chains differ from each other. Look at the local and quantile efficiency plots. You might try setting a higher value for the iter argument. By default iter is 2000 11.2.1.2 Bulk ESS Roughly speaking, the effective sample size (ESS) of a quantity of interest captures how many independent draws contain the same amount of information as the dependent sample obtained by the MCMC algorithm. Clearly, the higher the ESS the better. Stan uses R-hat adjustment to use the between-chain information in computing the ESS. For example, in case of multimodal distributions with well-separated modes, this leads to an ESS estimate that is close to the number of distinct modes that are found. Bulk-ESS refers to the effective sample size based on the rank normalized draws. This does not directly compute the ESS relevant for computing the mean of the parameter, but instead computes a quantity that is well defined even if the chains do not have finite mean or variance. Overall bulk-ESS estimates the sampling efficiency for the location of the distribution (e.g. mean and median). Often quite smaller ESS would be sufficient for the desired estimation accuracy, but the estimation of ESS and convergence diagnostics themselves require higher ESS. We recommend requiring that the bulk-ESS is greater than 100 times the number of chains. For example, when running four chains, this corresponds to having a rank-normalized effective sample size of at least 400. Recommendations: You might try setting a higher value for the iter argument. By default iter is 2000 Look at the rank plot to see how the chains differ from each other. Look at the local and quantile efficiency plots. Look at change in bulk-ESS when the number of iterations increase. If R-hat is less than 1.01 and bulk-ESS grows linearly with the number of iterations and eventually exceeds the recommended limit, the mixing is sufficient but MCMC has high autocorrelation requiring a large number of iterations 11.2.1.3 Tail ESS Tail-ESS computes the minimum of the effective sample sizes (ESS) of the 5% and 95% quantiles. Tail-ESS can help diagnosing problems due to different scales of the chains and slow mixing in the tails. See also general information about ESS above in description of bulk-ESS. Recommendations: You might try setting a higher value for the iter argument. By default iter is 2000 Look at the rank plot to see how the chains differ from each other. Look at the local and quantile efficiency plots. Look at change in tail-ESS when the number of iterations increase. If R-hat is less than 1.01 and tail-ESS grows linearly with the number of iterations and eventually exceeds the recommended limit, the mixing is sufficient but MCMC has high autocorrelation requiring a large number of iterations 11.2.1.4 Вернемся к нашим баранам Давайте посмотрим на наши данные: Оригинал мема. Вот еще один. 11.2.2 Модель только с угловым коэффициентом get_prior(n_tokens ~ n_words+0, data = ud) fit_slope &lt;- brm(n_tokens ~ n_words+0, data = ud, silent = TRUE) ## ## SAMPLING FOR MODEL &#39;63e6dde694613f0ad3384891aac6b5ee&#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 6.6e-05 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.66 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 1: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 1: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 1: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 1: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 1: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 1: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 1: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 1: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 1: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 1: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 1: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 0.274184 seconds (Warm-up) ## Chain 1: 0.242759 seconds (Sampling) ## Chain 1: 0.516943 seconds (Total) ## Chain 1: ## ## SAMPLING FOR MODEL &#39;63e6dde694613f0ad3384891aac6b5ee&#39; NOW (CHAIN 2). ## Chain 2: ## Chain 2: Gradient evaluation took 3.8e-05 seconds ## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.38 seconds. ## Chain 2: Adjust your expectations accordingly! ## Chain 2: ## Chain 2: ## Chain 2: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 2: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 2: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 2: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 2: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 2: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 2: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 2: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 2: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 2: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 2: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 2: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 2: ## Chain 2: Elapsed Time: 0.280093 seconds (Warm-up) ## Chain 2: 0.205477 seconds (Sampling) ## Chain 2: 0.48557 seconds (Total) ## Chain 2: ## ## SAMPLING FOR MODEL &#39;63e6dde694613f0ad3384891aac6b5ee&#39; NOW (CHAIN 3). ## Chain 3: ## Chain 3: Gradient evaluation took 3.7e-05 seconds ## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.37 seconds. ## Chain 3: Adjust your expectations accordingly! ## Chain 3: ## Chain 3: ## Chain 3: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 3: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 3: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 3: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 3: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 3: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 3: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 3: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 3: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 3: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 3: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 3: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 3: ## Chain 3: Elapsed Time: 0.264348 seconds (Warm-up) ## Chain 3: 0.233535 seconds (Sampling) ## Chain 3: 0.497883 seconds (Total) ## Chain 3: ## ## SAMPLING FOR MODEL &#39;63e6dde694613f0ad3384891aac6b5ee&#39; NOW (CHAIN 4). ## Chain 4: ## Chain 4: Gradient evaluation took 3.8e-05 seconds ## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.38 seconds. ## Chain 4: Adjust your expectations accordingly! ## Chain 4: ## Chain 4: ## Chain 4: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 4: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 4: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 4: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 4: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 4: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 4: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 4: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 4: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 4: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 4: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 4: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 4: ## Chain 4: Elapsed Time: 0.275579 seconds (Warm-up) ## Chain 4: 0.265569 seconds (Sampling) ## Chain 4: 0.541148 seconds (Total) ## Chain 4: fit_slope ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: n_tokens ~ n_words + 0 ## Data: ud (Number of observations: 20282) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## n_words 0.53 0.00 0.53 0.53 1.00 4057 2805 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 42.79 0.21 42.38 43.21 1.01 1048 980 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). plot(fit_slope) Давайте совместим предсказания модели и наши наблюдения. predict(fit_slope, newdata = tibble(n_words = 0:1500)) %&gt;% as_tibble() %&gt;% mutate(n_words = 0:1500) -&gt; fit_slope_predictions ud %&gt;% ggplot(aes(n_words, n_tokens))+ geom_smooth(data = fit_slope_predictions, aes(y = Estimate, ymin = Q2.5, ymax = Q97.5), stat = &quot;identity&quot;) + geom_point(alpha = 0.3) 11.2.3 Модель с угловым коэффициентом и свободным членом get_prior(n_tokens ~ n_words, data = ud) fit_slope_intercept &lt;- brm(n_tokens ~ n_words, data = ud) ## ## SAMPLING FOR MODEL &#39;c88af3a3b6ae974c4ea0204ec735207c&#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 4.2e-05 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.42 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 1: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 1: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 1: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 1: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 1: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 1: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 1: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 1: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 1: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 1: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 1: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 0.397296 seconds (Warm-up) ## Chain 1: 0.271143 seconds (Sampling) ## Chain 1: 0.668439 seconds (Total) ## Chain 1: ## ## SAMPLING FOR MODEL &#39;c88af3a3b6ae974c4ea0204ec735207c&#39; NOW (CHAIN 2). ## Chain 2: ## Chain 2: Gradient evaluation took 4e-05 seconds ## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.4 seconds. ## Chain 2: Adjust your expectations accordingly! ## Chain 2: ## Chain 2: ## Chain 2: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 2: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 2: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 2: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 2: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 2: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 2: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 2: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 2: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 2: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 2: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 2: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 2: ## Chain 2: Elapsed Time: 0.373286 seconds (Warm-up) ## Chain 2: 0.320569 seconds (Sampling) ## Chain 2: 0.693855 seconds (Total) ## Chain 2: ## ## SAMPLING FOR MODEL &#39;c88af3a3b6ae974c4ea0204ec735207c&#39; NOW (CHAIN 3). ## Chain 3: ## Chain 3: Gradient evaluation took 3.9e-05 seconds ## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.39 seconds. ## Chain 3: Adjust your expectations accordingly! ## Chain 3: ## Chain 3: ## Chain 3: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 3: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 3: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 3: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 3: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 3: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 3: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 3: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 3: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 3: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 3: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 3: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 3: ## Chain 3: Elapsed Time: 0.45014 seconds (Warm-up) ## Chain 3: 0.29954 seconds (Sampling) ## Chain 3: 0.74968 seconds (Total) ## Chain 3: ## ## SAMPLING FOR MODEL &#39;c88af3a3b6ae974c4ea0204ec735207c&#39; NOW (CHAIN 4). ## Chain 4: ## Chain 4: Gradient evaluation took 3.9e-05 seconds ## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.39 seconds. ## Chain 4: Adjust your expectations accordingly! ## Chain 4: ## Chain 4: ## Chain 4: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 4: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 4: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 4: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 4: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 4: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 4: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 4: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 4: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 4: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 4: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 4: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 4: ## Chain 4: Elapsed Time: 0.401435 seconds (Warm-up) ## Chain 4: 0.340333 seconds (Sampling) ## Chain 4: 0.741768 seconds (Total) ## Chain 4: fit_slope_intercept ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: n_tokens ~ n_words ## Data: ud (Number of observations: 20282) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 24.32 0.38 23.56 25.07 1.00 2603 2940 ## n_words 0.48 0.00 0.48 0.48 1.00 5476 2835 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 39.05 0.20 38.68 39.45 1.00 2004 1962 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). plot(fit_slope_intercept) predict(fit_slope_intercept, newdata = tibble(n_words = 0:1500)) %&gt;% as_tibble() %&gt;% mutate(n_words = 0:1500) -&gt; fit_slope_intercept_predictions ud %&gt;% ggplot(aes(n_words, n_tokens))+ geom_smooth(data = fit_slope_intercept_predictions, aes(y = Estimate, ymin = Q2.5, ymax = Q97.5), stat = &quot;identity&quot;) + geom_point(alpha = 0.3) 11.2.4 Модель со смешанными эффектами В данных есть группировка по языкам, которую мы все это время игнорировали. Давайте сделаем модель со смешанными эффектами: get_prior(n_tokens ~ n_words+(1|language), data = ud) fit_mixed &lt;- brm(n_tokens ~ n_words + (1|language), data = ud) ## ## SAMPLING FOR MODEL &#39;b59d4730b29e04c466a2875f317ee87f&#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 0.000725 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 7.25 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 1: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 1: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 1: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 1: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 1: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 1: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 1: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 1: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 1: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 1: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 1: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 148.025 seconds (Warm-up) ## Chain 1: 237.169 seconds (Sampling) ## Chain 1: 385.194 seconds (Total) ## Chain 1: ## ## SAMPLING FOR MODEL &#39;b59d4730b29e04c466a2875f317ee87f&#39; NOW (CHAIN 2). ## Chain 2: ## Chain 2: Gradient evaluation took 0.000556 seconds ## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 5.56 seconds. ## Chain 2: Adjust your expectations accordingly! ## Chain 2: ## Chain 2: ## Chain 2: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 2: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 2: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 2: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 2: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 2: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 2: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 2: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 2: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 2: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 2: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 2: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 2: ## Chain 2: Elapsed Time: 215.539 seconds (Warm-up) ## Chain 2: 275.448 seconds (Sampling) ## Chain 2: 490.988 seconds (Total) ## Chain 2: ## ## SAMPLING FOR MODEL &#39;b59d4730b29e04c466a2875f317ee87f&#39; NOW (CHAIN 3). ## Chain 3: ## Chain 3: Gradient evaluation took 0.00056 seconds ## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 5.6 seconds. ## Chain 3: Adjust your expectations accordingly! ## Chain 3: ## Chain 3: ## Chain 3: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 3: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 3: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 3: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 3: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 3: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 3: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 3: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 3: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 3: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 3: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 3: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 3: ## Chain 3: Elapsed Time: 194.498 seconds (Warm-up) ## Chain 3: 194.901 seconds (Sampling) ## Chain 3: 389.398 seconds (Total) ## Chain 3: ## ## SAMPLING FOR MODEL &#39;b59d4730b29e04c466a2875f317ee87f&#39; NOW (CHAIN 4). ## Chain 4: ## Chain 4: Gradient evaluation took 0.000556 seconds ## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 5.56 seconds. ## Chain 4: Adjust your expectations accordingly! ## Chain 4: ## Chain 4: ## Chain 4: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 4: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 4: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 4: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 4: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 4: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 4: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 4: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 4: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 4: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 4: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 4: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 4: ## Chain 4: Elapsed Time: 200.121 seconds (Warm-up) ## Chain 4: 196.632 seconds (Sampling) ## Chain 4: 396.752 seconds (Total) ## Chain 4: ## Warning: There were 4 divergent transitions after warmup. See ## https://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup ## to find out why this is a problem and how to eliminate them. ## Warning: There were 155 transitions after warmup that exceeded the maximum treedepth. Increase max_treedepth above 10. See ## https://mc-stan.org/misc/warnings.html#maximum-treedepth-exceeded ## Warning: Examine the pairs() plot to diagnose sampling problems fit_mixed ## Warning: There were 4 divergent transitions after warmup. Increasing adapt_delta ## above 0.8 may help. See http://mc-stan.org/misc/warnings.html#divergent- ## transitions-after-warmup ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: n_tokens ~ n_words + (1 | language) ## Data: ud (Number of observations: 20282) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Group-Level Effects: ## ~language (Number of levels: 9) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 64.09 18.60 37.81 109.84 1.01 713 955 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 7.47 20.55 -32.80 48.19 1.01 598 760 ## n_words 0.49 0.00 0.49 0.49 1.00 3983 2590 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 29.29 0.15 28.99 29.59 1.00 2097 1940 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). plot(fit_mixed) new_data &lt;- tibble(n_words = rep(0:1500, length(unique(ud$language))), language = rep(unique(ud$language), each = 1501)) predict(fit_mixed, newdata = new_data) %&gt;% as_tibble() %&gt;% bind_cols(new_data) -&gt; fit_mixed_predictions ud %&gt;% ggplot(aes(n_words, n_tokens))+ geom_smooth(data = fit_mixed_predictions, aes(y = Estimate, ymin = Q2.5, ymax = Q97.5), stat = &quot;identity&quot;) + geom_point(alpha = 0.3)+ facet_wrap(~language) То, что получилось учитывает общий эффект: посмотрите на каталанский. Если построить модель по каждому языку, то получится совсем другая картина: ud %&gt;% ggplot(aes(n_words, n_tokens))+ geom_smooth(method = &quot;lm&quot;) + geom_point(alpha = 0.3)+ facet_wrap(~language) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
